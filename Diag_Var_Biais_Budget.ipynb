{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance et biais en fonction du budget\n",
    "\n",
    "Tracer en fonction du budget :\n",
    "- Variance ML\n",
    "- Biais ML\n",
    "- Variance LML\n",
    "- Biais LML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import cupy as np # Oui je sais c'est une abomination mais c'est juste pour pas à tout rechanger si on change de PC\n",
    "import numpy as npp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "np.fuse(npp.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_vec(X,v):\n",
    "    return v.T@X\n",
    "\n",
    "def f_vec_squared(X,v):\n",
    "    return (v.T@X)**2\n",
    "\n",
    "d = 10 # lenght of random vectors\n",
    "\n",
    "v_1= np.array([-0.00606533, -0.02837768, -0.20481078, -0.05524456,  0.00408442, -0.02378791, -0.11289296, -0.09047946, -0.0828985,   0.01015773])\n",
    "# eps = np.random.normal(0,0.001,d) \n",
    "# v_0 = v_1 + eps  #vecteur ceof basse fidélité \n",
    "# print(v_0)\n",
    "# v_0 = np.array([-0.00589986, -0.02848255, -0.2058163,  -0.05332501,  0.00356343, -0.02399142, -0.11355733, -0.08986586, -0.08302782,  0.0085727 ])\n",
    "v_0 = np.array([-0.04775578, -0.02158142, -0.22861181,  0.00999135 , 0.05261623, -0.03810132, -0.08892537, -0.09858141, -0.06851002,  0.09957945])\n",
    "exp_var = np.sum(np.square(v_1))\n",
    "\n",
    "print(\"Expected variance =\", exp_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Tenseur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f_vec_squared_tensor(X,v):\n",
    "#     '''\n",
    "#     Au final, fonction inutile car f_vec_squared s'adapte, c'est fou\n",
    "#     '''\n",
    "#     X_reorganized = np.transpose(X, (0,2,1))\n",
    "    \n",
    "#     result = np.einsum('ijk,k->ij', X_reorganized, v)\n",
    "#     return result ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = 10\n",
    "# M_0 = 17\n",
    "# N = 10000\n",
    "# X = np.random.standard_normal((N,d,M_0))\n",
    "# X0 = np.random.standard_normal((N,d,M_0))\n",
    "\n",
    "# # Y_bis = f_vec_squared_tensor(X,v_1)\n",
    "# Y = f_vec_squared(X,v_1)\n",
    "# Y0 = f_vec_squared(X,v_0)\n",
    "# (Y-Y0).shape\n",
    "\n",
    "# #np.var(np.mean(Y, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif : remplacer nos boucles for par des tenseurs pour direct faire des produits matriciels avec Numpy et ganger du temps de calcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estim_var_MLMC(N,M_0,M_1):\n",
    "    X1 = np.random.standard_normal((N,d,M_0))\n",
    "    X0 = np.random.standard_normal((N,d,M_1))\n",
    "\n",
    "    #estimateur de la variance \n",
    "    Y0_0_squared = f_vec_squared(X0,v_0)\n",
    "    Y1_0_squared = f_vec_squared(X1,v_0)\n",
    "    Y1_1_squared = f_vec_squared(X1,v_1)\n",
    "    Var_MLMC = np.mean(Y0_0_squared, axis=1) + np.mean(Y1_1_squared-Y1_0_squared, axis=1)\n",
    "\n",
    "    return Var_MLMC\n",
    "\n",
    "def estim_var_MLMC_log(N,M_0,M_1):\n",
    "    X1 = np.random.standard_normal((N,d,M_0))\n",
    "    X0 = np.random.standard_normal((N,d,M_1))\n",
    "\n",
    "    Y0_0_squared = f_vec_squared(X0,v_0)\n",
    "    Y1_0_squared = f_vec_squared(X1,v_0)\n",
    "    Y1_1_squared = f_vec_squared(X1,v_1)\n",
    "    Var_MLMC_log = np.exp(np.log(np.mean(Y0_0_squared, axis=1)) + np.log(np.mean(Y1_1_squared, axis=1))-np.log(np.mean(Y1_0_squared, axis=1)))\n",
    "    return Var_MLMC_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance et biais en fonction du budget :\n",
    "\n",
    "#### Calcul des $M_\\ell$ : \n",
    "\n",
    "Calcul de $M_\\ell = m + 1 + \\lfloor (\\tilde{\\eta} / \\mathcal{S}_L) \\sqrt{\\mathcal{V}_\\ell / \\mathcal{C}_\\ell} \\rfloor > m $\n",
    "\n",
    "avec : \n",
    "$$\n",
    " \\tilde{\\eta} = \\eta - m \\sum_{\\ell' \\le L} \\mathcal{C}_{\\ell'} \\\\\n",
    " \\mathcal{S}_L = \\sum_{\\ell \\le L} \\sqrt{\\mathcal{V}_\\ell \\mathcal{C}_\\ell}\n",
    " $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_ell_tab(eta, V_ell, C_ell,m=0):\n",
    "    \"\"\"\n",
    "    eta : budget\n",
    "    V_ell : tableau contenant [V_0,V_1,...,V_L]\n",
    "    C_ell : tableau contenant [C_0,C_1,...,C_L]\n",
    "\n",
    "    return : la formule M_ell au dessus qui nous donne le nombre d'élement optimal à simuler\n",
    "    \"\"\"\n",
    "    eta_tilde = eta # - m*np.sum(...)\n",
    "    S_L = np.sum(np.sqrt(np.multiply(V_ell,C_ell)))\n",
    "\n",
    "    return m + 1 + np.floor(\n",
    "        (eta_tilde/S_L)*np.sqrt(V_ell*1/C_ell)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Définition des constantes : \n",
    "\n",
    "Ici, nous avons :\n",
    "$$\n",
    "m = 0 \\\\\n",
    "\\mathcal{C}_1 = 1 \\\\\n",
    "\\mathcal{C}_0 = 0.5 \\\\\n",
    "\\eta = \\text{varie de 10 à } 10^5\n",
    "$$\n",
    "\n",
    "De plus, on calcul nos $V_\\ell$ avec un echantillon pilote de $N_{pilote} = 10000$.\n",
    "\n",
    "Enfin, pour estimer le biais et la variance de nos estimateurs de la variance multi-level $V^{ML}_L$ et $V^{LogML}_L$, on prend une taille de $N = 10000$ (On génère $N$ valeurs de $V^{ML}_L$ et $V^{LogML}_L$ pour ensuite calculer leur variance et leur biais)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pilote = 10000\n",
    "X = np.random.standard_normal((d,N_pilote))\n",
    "V_0 = np.var(f_vec_squared(X,v_0))\n",
    "V_1 = np.var(f_vec_squared(X,v_1) - f_vec_squared(X,v_0))\n",
    "\n",
    "V_ell = np.array([V_0,V_1])\n",
    "C_ell = np.array([0.75,1.])\n",
    "\n",
    "N=10000\n",
    "budget_eta = npp.logspace(1,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_tab = []\n",
    "var_MLMC_tab = []\n",
    "bias_log_tab = []\n",
    "var_MLMC_log_tab = []\n",
    "\n",
    "for eta in tqdm(budget_eta):\n",
    "    # Pour chaque budget eta, calcul de nos M_0 et M_1\n",
    "    M_0,M_1 = map(int, M_ell_tab(eta,V_ell,C_ell))\n",
    "\n",
    "    # Calcul de nos tableaux de nos estimateurs de taille N\n",
    "    Tab_var_MLMC = estim_var_MLMC(N,M_0,M_1)\n",
    "    var_MLMC_tab.append(np.var(Tab_var_MLMC))\n",
    "    bias_tab.append(np.square(np.mean(Tab_var_MLMC) - exp_var))\n",
    "\n",
    "    Tab_var_MLMC = None\n",
    "    np.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    Tab_var_MLMC_log = estim_var_MLMC_log(N,M_0,M_1)\n",
    "    var_MLMC_log_tab.append(np.var(Tab_var_MLMC_log))\n",
    "    bias_log_tab.append(np.square(np.mean(Tab_var_MLMC_log) - exp_var))\n",
    "\n",
    "    Tab_var_MLMC_log = None\n",
    "    np.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "var_MLMC_tab = [x.item() for x in var_MLMC_tab]\n",
    "bias_tab = [x.item() for x in bias_tab]\n",
    "\n",
    "var_MLMC_log_tab = [x.item() for x in var_MLMC_log_tab]\n",
    "bias_log_tab = [x.item() for x in bias_log_tab]\n",
    "\n",
    "# Temps à battre avec deux boucle 'for' : 12m50\n",
    "# avec les paramètres suivant :\n",
    "# budget_eta = np.logspace(1,5,10)\n",
    "# N=10000\n",
    "    \n",
    "# Temps à battre avec deux boucle 'for' : 1m37\n",
    "# avec les paramètres suivant :\n",
    "# budget_eta = np.logspace(1,4,10)\n",
    "# N=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log-log\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot des données\n",
    "plt.loglog(budget_eta, bias_tab, marker='o', label='Biais carré Tab')\n",
    "plt.loglog(budget_eta, var_MLMC_tab, marker='s', label='Var MLMC Tab')\n",
    "plt.loglog(budget_eta, bias_log_tab, marker='^', label='Biais carré Log Tab')\n",
    "plt.loglog(budget_eta, var_MLMC_log_tab, marker='x', label='Var MLMC Log Tab')\n",
    "\n",
    "# Ajouter titre et légendes\n",
    "plt.title('Graphique Log-Log')\n",
    "plt.xlabel('Budget eta')\n",
    "plt.ylabel('Valeurs')\n",
    "plt.legend()\n",
    "\n",
    "# Afficher le plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "free_memory = np.cuda.Device().mem_info[0]\n",
    "print(\"Mémoire libre sur le périphérique GPU :\", free_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet4A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
