{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "cp.fuse(np.float32)\n",
    "h = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v= cp.array([-0.00606533, -0.02837768, -0.20481078, -0.05524456,  0.00408442, -0.02378791, -0.11289296, -0.09047946, -0.0828985,   0.01015773]) # Real life\n",
    "v_0 = cp.array([-0.26251362, -0.22397083, -0.28459696, -0.14160629,  0.11507459,\n",
    "       -0.01314795,  0.00368215, -0.2233519 , -0.0494188 , -0.09833207])\n",
    "v_0 = cp.array([ 1.02602951,  1.59187996,  0.23337749, -0.62634188,  0.16052645,\n",
    "       -0.47135837,  0.21609987,  0.45909724,  0.70707697, -0.76436048])\n",
    "v_1 = cp.array([-0.09152057,  0.26501426, -0.26361748, -0.09584528,  0.07564258,\n",
    "       -0.28932995, -0.15172387, -0.17311338, -0.02250007, -0.02662765])\n",
    "v_2 = cp.array([ 0.02806491,  0.18164134, -0.02569311,  0.08406244,  0.09760685,\n",
    "       -0.2754576 , -0.18692242,  0.05429335, -0.05959692, -0.16104073])\n",
    "v_3 = cp.array([ 0.00719622, -0.01367537, -0.04378771,  0.15642576,  0.03295938,\n",
    "       -0.1364489 , -0.02709714, -0.16822205, -0.15617831, -0.05832736])\n",
    "\n",
    "v_array = [v_0, v_1, v_2, v_3]\n",
    "exp_var = cp.sum(cp.square(v))\n",
    "def expected_var(v):\n",
    "    return cp.sum(cp.square(v))\n",
    "\n",
    "# Get the current free memory on CUDA device\n",
    "free_memory = cp.cuda.Device().mem_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating vectors to have a variance close to the real one v\n",
    "eps = 7e-1\n",
    "v_generated = v - cp.random.normal(0,eps,len(v))\n",
    "print(cp.linalg.norm(expected_var(v_generated) - expected_var(v)))\n",
    "v_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable & functions def: \n",
    "def f_vec(X,v):\n",
    "    return v.T@X\n",
    "\n",
    "def f_vec_squared(X,v):\n",
    "    return (v.T@X)**2\n",
    "\n",
    "d = 10 # lenght of random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the estimator of the variance in ML & LML :\n",
    "def estim_var_MLMC(N, d, M, batch_size):\n",
    "    var_results = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        Var_MLMC_batch = cp.var(Y_ell_M_ell, axis=1, ddof=1)\n",
    "    \n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            Var_MLMC_batch += cp.var(Y_ell_M_ell, axis=1, ddof=1) - cp.var(Y_ell_minus_1_M_ell, axis=1, ddof=1)\n",
    "\n",
    "        var_results.extend(Var_MLMC_batch)\n",
    "\n",
    "        # Free memory\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results)\n",
    "\n",
    "def estim_var_MLMC_log(N, d, M, batch_size):\n",
    "    var_results = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        log_Var_log_MLMC = cp.log(cp.var(Y_ell_M_ell, axis=1, ddof=1))\n",
    "    \n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            log_Var_log_MLMC += np.log(cp.var(Y_ell_M_ell, axis=1, ddof=1)) - cp.log(cp.var(Y_ell_minus_1_M_ell, axis=1, ddof=1))\n",
    "\n",
    "        Var_MLMC_log_batch = cp.exp(log_Var_log_MLMC)\n",
    "    \n",
    "        var_results.extend(Var_MLMC_log_batch)\n",
    "\n",
    "        # Free memory if necessary\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we can't compute all at once due to M_0 and M_1 increasing, we need to separate the caluclations by computing batch_size first\n",
    "def get_max_batch_size(d, M, free_memory, memory_utilization=0.8):\n",
    "    \"\"\"\n",
    "    Calculate the maximum batch size directly based on the memory usage per data sample and the available memory.\n",
    "    \"\"\"\n",
    "    # Calculate the memory usage per sample\n",
    "    bytes_per_number = cp.dtype('float32').itemsize\n",
    "    omega = (d+1) * bytes_per_number * (np.sum(M))  # Total memory used per sample\n",
    "\n",
    "    # Calculate the maximum amount of memory available for batches\n",
    "    max_memory_per_batch = free_memory * memory_utilization\n",
    "\n",
    "    # Calculate the maximum batch size that can fit within the memory limit\n",
    "    batch_size = int(max_memory_per_batch / omega)  # Use int to ensure we get a whole number\n",
    "\n",
    "    return batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "M = [18]\n",
    "X_M_ell = cp.random.standard_normal((N,d,M[0]))\n",
    "Y_ell_M_ell_squared = f_vec_squared(X_M_ell,v)\n",
    "Y_ell_M_ell = f_vec(X_M_ell,v)\n",
    "\n",
    "print(cp.mean(Y_ell_M_ell))\n",
    "\n",
    "print(\"Var en calculant avec cp.var (ddof = 1) =\", cp.var(Y_ell_M_ell, axis=1, ddof=1))\n",
    "print(\"Var en calculant avec cp.var + correction =\", M[0]/(M[0]-1)*cp.var(Y_ell_M_ell, axis=1))\n",
    "print(\"var en calculant à la main la somme =\", cp.sum(Y_ell_M_ell_squared, axis=1)/(M[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_ell_tab(eta, V_ell, C_ell,m=0):\n",
    "    \"\"\"\n",
    "    eta : budget\n",
    "    V_ell : tableau contenant [V_0,V_1,...,V_L]\n",
    "    C_ell : tableau contenant [C_0,C_1,...,C_L]\n",
    "\n",
    "    return : la formule M_ell au dessus qui nous donne le nombre d'élement optimal à simuler\n",
    "    \"\"\"\n",
    "    eta_tilde = eta # - m*cp.sum(...)\n",
    "    S_L = (cp.sqrt(cp.multiply(V_ell,C_ell))).sum()\n",
    "\n",
    "    return m + 1 + cp.floor(\n",
    "        (eta_tilde/S_L)*cp.sqrt(V_ell*1/C_ell)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pilote = 10000\n",
    "X = cp.random.standard_normal((d,N_pilote))\n",
    "V = [cp.var(f_vec_squared(X,v_0))]\n",
    "for i in range(1,len(v_array)):\n",
    "    V.extend([cp.var(f_vec_squared(X,v_array[i]) - f_vec_squared(X,v_array[i-1]))])\n",
    "\n",
    "V_ell = cp.array(V)\n",
    "C_ell = cp.array([.25,0.5,0.75,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=5000\n",
    "budget_eta = np.logspace(1,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_tab = []\n",
    "var_MLMC_tab = []\n",
    "bias_log_tab = []\n",
    "var_MLMC_log_tab = []\n",
    "\n",
    "\n",
    "for eta in tqdm(budget_eta):\n",
    "    # Pour chaque budget eta, calcul de nos M_0 et M_1\n",
    "    M = list(map(int,M_ell_tab(eta,V_ell,C_ell)))\n",
    "\n",
    "    # Compute the batch size\n",
    "    batch_size = get_max_batch_size(d, M, free_memory)\n",
    "    if batch_size == 0:\n",
    "        break  # Exit if batch size is too small\n",
    "    \n",
    "    # Calcul de nos tableaux de nos estimateurs de taille N\n",
    "    Tab_var_MLMC = estim_var_MLMC(N, d, M, batch_size)\n",
    "    var_MLMC_tab.append(cp.var(Tab_var_MLMC))\n",
    "    bias_tab.append(cp.square(cp.mean(Tab_var_MLMC) - expected_var(v)))\n",
    "\n",
    "    Tab_var_MLMC = None\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    Tab_var_MLMC_log = estim_var_MLMC_log(N, d, M, batch_size)\n",
    "    var_MLMC_log_tab.append(cp.var(Tab_var_MLMC_log))\n",
    "    bias_log_tab.append(cp.square(cp.mean(Tab_var_MLMC_log) - expected_var(v)))\n",
    "\n",
    "    Tab_var_MLMC_log = None\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "var_MLMC_tab = [x.item() for x in var_MLMC_tab]\n",
    "bias_tab = [x.item() for x in bias_tab]\n",
    "\n",
    "var_MLMC_log_tab = [x.item() for x in var_MLMC_log_tab]\n",
    "bias_log_tab = [x.item() for x in bias_log_tab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log-log\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot des données\n",
    "plt.loglog(budget_eta, bias_tab, marker='o', label='Bias^2 MLMC')\n",
    "plt.loglog(budget_eta, var_MLMC_tab, marker='s', label='Var MLMC')\n",
    "plt.loglog(budget_eta, bias_log_tab, marker='^', label='Bias^2 Log-MLMC')\n",
    "plt.loglog(budget_eta, var_MLMC_log_tab, marker='x', label='Var Log-MLMC')\n",
    "\n",
    "\n",
    "# Ajouter titre et légendes\n",
    "plt.title(f'Biais^2 et variance en fonction du budget pour n={N}')\n",
    "plt.xlabel('Budget eta')\n",
    "plt.ylabel('Bias^2/Variance')\n",
    "plt.legend()\n",
    "\n",
    "# Afficher le plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((exp_var - expected_var(v_3))**2)\n",
    "print(bias_log_tab[-1])\n",
    "print(bias_tab[2])\n",
    "bias_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_log_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE en fonction de n\n",
    "budget fixé à $10^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_eta_int = 1e3\n",
    "samples_N = list(map(int,np.logspace(2,5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_tab = []\n",
    "var_MLMC_tab = []\n",
    "bias_log_tab = []\n",
    "var_MLMC_log_tab = []\n",
    "\n",
    "\n",
    "for N in tqdm(samples_N):\n",
    "    # Pour chaque budget eta, calcul de nos M_0 et M_1\n",
    "    M = list(map(int,M_ell_tab(budget_eta_int,V_ell,C_ell)))\n",
    "\n",
    "    # Compute the batch size\n",
    "    batch_size = get_max_batch_size(d, M, free_memory)\n",
    "    if batch_size == 0:\n",
    "        break  # Exit if batch size is too small\n",
    "    \n",
    "    # Calcul de nos tableaux de nos estimateurs de taille N\n",
    "    Tab_var_MLMC = estim_var_MLMC(N, d, M, batch_size)\n",
    "    var_MLMC_tab.append(cp.var(Tab_var_MLMC))\n",
    "    bias_tab.append(cp.square(cp.mean(Tab_var_MLMC) - expected_var(v)))\n",
    "\n",
    "    Tab_var_MLMC = None\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    Tab_var_MLMC_log = estim_var_MLMC_log(N, d, M, batch_size)\n",
    "    var_MLMC_log_tab.append(cp.var(Tab_var_MLMC_log))\n",
    "    bias_log_tab.append(cp.square(cp.mean(Tab_var_MLMC_log) - expected_var(v)))\n",
    "\n",
    "    Tab_var_MLMC_log = None\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "var_MLMC_tab = [x.item() for x in var_MLMC_tab]\n",
    "bias_tab = [x.item() for x in bias_tab]\n",
    "\n",
    "var_MLMC_log_tab = [x.item() for x in var_MLMC_log_tab]\n",
    "bias_log_tab = [x.item() for x in bias_log_tab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log-log\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot des données\n",
    "plt.loglog(samples_N, bias_tab, marker='o', label='MSE MLMC')\n",
    "#plt.loglog(samples_N, var_MLMC_tab, marker='s', label='Var MLMC')\n",
    "plt.loglog(samples_N, bias_log_tab, marker='^', label='MSE Log-MLMC', c='g')\n",
    "#plt.loglog(samples_N, var_MLMC_log_tab, marker='x', label='Var Log-MLMC')\n",
    "\n",
    "\n",
    "# Ajouter titre et légendes\n",
    "plt.title(f'MSE et variance en fonction des N pour budget={budget_eta_int}')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('MSE/Variance')\n",
    "plt.legend()\n",
    "\n",
    "# Afficher le plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teta_hat = cp.random.exponential(30,10)\n",
    "cp.mean(cp.log(teta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.log(cp.mean(teta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.random.exponential(2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation du Bootstrap dans le calcul à chaque niveau\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbb V = \\log \\left( x \\right)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bootstrap_var(Y, num_samples):\n",
    "    \"\"\"\n",
    "    This function computes the variance of Y using the bootstrap method.\n",
    "    It creates num_samples of variances by taking randomly M_ell (= len(Y)) values in Y, and then returning its variance.\n",
    "    Then it return the expectation of this variances\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: numpy array, values that we want the variance\n",
    "    - num_samples: int, the number of bootstrap samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    - int, the expectation of bootstrap variances \n",
    "    \"\"\"\n",
    "    N, M_ell = cp.shape(Y)\n",
    "    bootstrap_variances = cp.empty((N,num_samples))\n",
    "    for i in range(num_samples):\n",
    "        sample_indices = np.random.randint(0, M_ell, M_ell)\n",
    "        bootstrap_Y = Y[:, sample_indices]\n",
    "        bootstrap_variances[:,i] = cp.var(bootstrap_Y, axis=1, ddof=1) # corrected empirical variance\n",
    "\n",
    "    return cp.mean(bootstrap_variances, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_bootstrap_var(Y, num_samples):\n",
    "    \"\"\"\n",
    "    This function computes the variance of Y using the bootstrap method, vectorized to avoid explicit loops.\n",
    "    It creates num_samples of variances by taking randomly M_ell (= len(Y)) values in Y, and then returns its variance.\n",
    "    Then it returns the expectation of these variances.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: numpy array, values for which we want the variance\n",
    "    - num_samples: int, the number of bootstrap samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array, the expectation of bootstrap variances for each feature\n",
    "    \"\"\"\n",
    "    N, M_ell = Y.shape\n",
    "    # Generate random indices for all bootstrap samples at once\n",
    "    sample_indices = np.random.randint(0, M_ell, (M_ell, num_samples))\n",
    "    # Index Y to create the bootstrap samples: (N, M_ell, num_samples)\n",
    "    bootstrap_Y = Y[:, sample_indices]\n",
    "    # Compute variances across the middle dimension, for each sample\n",
    "    bootstrap_variances = np.var(bootstrap_Y, axis=1, ddof=1)\n",
    "    # Compute the mean of the variances across samples\n",
    "    mean_variances = np.mean(bootstrap_variances, axis=1)\n",
    "    return mean_variances\n",
    "\n",
    "# Uncomment the following line to test the function with example data\n",
    "# vectorized_bootstrap_var(np.random.rand(10, 100), 1000)  # Example call with random data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the estimator of the variance in ML & LML :\n",
    "def Estim_var_MLMC_bootstrap(N, d, M, batch_size, bootstrap_sample_size=1000):\n",
    "    var_results = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        Var_MLMC_batch = Bootstrap_var(Y_ell_M_ell,bootstrap_sample_size)\n",
    "    \n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            V_ell_M_ell_bootstrap = Bootstrap_var(Y_ell_M_ell,bootstrap_sample_size)\n",
    "\n",
    "            Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            V_ell_minus_1_M_ell_bootstrap = Bootstrap_var(Y_ell_minus_1_M_ell,bootstrap_sample_size)\n",
    "\n",
    "            Var_MLMC_batch += V_ell_M_ell_bootstrap - V_ell_minus_1_M_ell_bootstrap\n",
    "\n",
    "        var_results.extend(Var_MLMC_batch)\n",
    "\n",
    "        # Free memory\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results)\n",
    "\n",
    "def Estim_var_MLMC_log(N, d, M, batch_size, bootstrap_sample_size):\n",
    "    var_results = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        log_Var_log_MLMC = cp.log(cp.var(Y_ell_M_ell, axis=1, ddof=1))\n",
    "    \n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            log_Var_log_MLMC += np.log(cp.var(Y_ell_M_ell, axis=1, ddof=1)) - cp.log(cp.var(Y_ell_minus_1_M_ell, axis=1, ddof=1))\n",
    "\n",
    "        Var_MLMC_log_batch = cp.exp(log_Var_log_MLMC)\n",
    "    \n",
    "        var_results.extend(Var_MLMC_log_batch)\n",
    "\n",
    "        # Free memory if necessary\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "budget_eta_int_bootstrap = 1e3\n",
    "M = list(map(int,M_ell_tab(budget_eta_int_bootstrap,V_ell,C_ell)))\n",
    "\n",
    "batch_size = get_max_batch_size(d, M, free_memory)\n",
    "if batch_size == 0:\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "Tab_var_MLMC = estim_var_MLMC(N, d, M, batch_size)\n",
    "\n",
    "Tab_var_MLMC_bootstrap = Estim_var_MLMC_bootstrap(N, d, M, batch_size, bootstrap_sample_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab_var_MLMC.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab_var_MLMC_bootstrap.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_var(v_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet4A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
