{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMC Estimation of the covariance (matrix 2x2)\n",
    "\n",
    "Lets try to compute the MLMC Estimation of the covariance $\\Sigma = \\text{Cov(y)}$\n",
    "\n",
    "### Defintions :\n",
    "As told in \"Multilevel Monte Carlo covariance estimation for the computation of Sobol’ indices\" ([Myzek & de Lozzo](https://hal.science/hal-01894503/document), (2.1)) We consider an abstract numerical simulator described by the function :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f_\\ell : \\mathcal{X}=\\mathbb{R}^d & \\rightarrow \\mathbb{R} \\\\\n",
    "    X & \\mapsto (v^T \\cdot X , w^T \\cdot X), \\quad v,w \\in \\mathbb{R}^d, \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We note $Y_\\ell = (Y_{\\ell,1},Y_{\\ell,2}) =f_\\ell(X)$.\n",
    "\n",
    "Let $Y_L = (Y_{L,1},Y_{L,2}) =f_L(X)$ the random vector based on the numerical simulator with the highest fidelity $L > \\ell$.\n",
    "\n",
    "We have :\n",
    "$$\\Sigma =\n",
    "\\text{cov}(Y_L) = \n",
    "\\begin{bmatrix}\n",
    "\\text{cov}(Y_{L,1}, Y_{L,1}) & \\text{cov}(Y_{L,1}, Y_{L,2}) & \\cdots & \\text{cov}(Y_{L,1}, Y_{L,d}) \\\\\n",
    "\\text{cov}(Y_{L,2}, Y_{L,1}) & \\text{cov}(Y_{L,2}, Y_{L,2}) & \\cdots & \\text{cov}(Y_{L,2}, Y_{L,d}) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{cov}(Y_{L,d}, Y_{L,1}) & \\text{cov}(Y_{L,d}, Y_{L,2}) & \\cdots & \\text{cov}(Y_{L,d}, Y_{L,d}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<details>\n",
    "$$\\Sigma =\n",
    "\\begin{bmatrix}\n",
    "\\text{Var}(Y_1) & \\text{cov}(Y_1,Y_2) \\\\\n",
    "\\text{cov}(Y_2,Y_1) & \\text{Var}(Y_2) \\\\\n",
    "\\end{bmatrix}$$\n",
    "</details>\n",
    "\n",
    "\n",
    "#### Sampling\n",
    "\n",
    "Let $\\ell \\in \\{ 1, \\dots, L\\}$ corresponding of the level of fidelity of our numerical simulator $f_\\ell$.\n",
    "\n",
    "We have $n_0 > n_1 > \\dots > n_\\ell > \\dots > n_L$.\n",
    "\n",
    "We now take a sample of $n$ elements : $\\left(X^{(1)},\\dots,X^{(n)}\\right)_{n\\in\\mathbb{N}}$ which gives us for each level of fidelity $\\ell$ :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(Y_\\ell^{(1)},\\dots,Y_\\ell^{(n_\\ell)}\\right) & = \\left(f_\\ell(X^{(1)}), \\dots, f_\\ell(X^{(n_\\ell)}) \\right) \\\\\n",
    "& = \\left( \\begin{pmatrix} Y_{\\ell,1}^{(1)} \\\\[7pt] Y_{\\ell,2}^{(1)} \\end{pmatrix}, \\dots, \\begin{pmatrix} Y_{\\ell,1}^{(n_\\ell)} \\\\[7pt] Y_{\\ell,2}^{(n_\\ell)} \\end{pmatrix}  \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Once we have this sample, we want to compute $\\hat{\\Sigma}_{\\ell}$. So for each level of fidelity $\\ell$, we have :\n",
    "\n",
    "1. Let's compute the empirical mean $\\hat{\\mu}_{\\ell}$ of each component of the random vector by taking the mean of each component over the whole sample. For $d$ components, we get $d$ empirical means :\n",
    "    $$\n",
    "    \\hat{\\mu}_{\\ell,i} = \\frac{1}{n_{\\ell}} \\sum_{k=1}^{n_{\\ell}}(Y_{\\ell,i}^{(k)})\n",
    "    $$\n",
    "    Here, $i \\in \\{1,2\\}$ because $d=2$ in our case. So we have 2 empirical means.\n",
    "\n",
    "2. Then we estimate the covariances for all $(i,j) \\in \\{1,\\dots,d \\}$ :\n",
    "    $$\n",
    "    \\hat{\\Sigma}_{\\ell,ij} = \\frac{1}{n_\\ell} \\sum_{k=1}^{n_\\ell}\\left(Y_{\\ell,i}^{(k)}-\\hat{\\mu}_{\\ell,i} \\right)\\left(Y_{\\ell,j}^{(k)}-\\hat{\\mu}_{\\ell,j} \\right)\n",
    "    $$\n",
    "\n",
    "    We finally have our $(d \\times d)$ matrix :\n",
    "    $$\n",
    "    \\hat{\\Sigma}_\\ell =\n",
    "    \\begin{bmatrix}\n",
    "    \\hat{\\Sigma}_{\\ell,11} & \\hat{\\Sigma}_{\\ell,12} & \\cdots & \\hat{\\Sigma}_{\\ell,1d} \\\\\n",
    "    \\hat{\\Sigma}_{\\ell,21} & \\hat{\\Sigma}_{\\ell,22} & \\cdots & \\hat{\\Sigma}_{\\ell,2d} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\hat{\\Sigma}_{\\ell,d1} & \\hat{\\Sigma}_{\\ell,d2} & \\cdots & \\hat{\\Sigma}_{\\ell,dd} \\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    Which, in our case for $d=2$, is :\n",
    "\n",
    "    $$\\hat{\\Sigma}_\\ell =\n",
    "    \\begin{bmatrix}\n",
    "    \\hat{\\Sigma}_{\\ell,11}  & \\hat{\\Sigma}_{\\ell,12} \\\\\n",
    "    \\hat{\\Sigma}_{\\ell,21}  & \\hat{\\Sigma}_{\\ell,22} \\\\\n",
    "    \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classic Estimator of the covariance matrix :\n",
    "\n",
    "To estimate $\\Sigma$ by the classic method, we just do what's on the Sampling section with $\\ell = L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MLMC Covariance Estimation in Euclidean Geometry\n",
    "\n",
    "Based on the work of Maurais Aimee, Terrence Alsup, Benjamin Peherstorfer, and Youssef Marzouk. [“Multi-Fidelity Covariance Estimation in the Log-Euclidean Geometry.”](https://arxiv.org/pdf/2301.13749.pdf) **\"Section 2\"** and in \"Multilevel Monte Carlo covariance estimation for the computation of Sobol’ indices\" ([Myzek & de Lozzo](https://hal.science/hal-01894503/document), (2.22, 2.23, 2.24)) we have the MLMC Covariance Estimation in Euclidian Geometry noted $\\hat{\\Sigma}_{L}^{EMF}$ :\n",
    "\n",
    "$$\n",
    "\\hat{\\Sigma}_{L}^{EMF} = \\hat{\\Sigma}_0[Y_0] + \\sum_{l=1}^{L}\\left(\\hat{\\Sigma}_{n_\\ell} [Y_\\ell] - \\hat{\\Sigma}_{n_\\ell}[Y_{\\ell-1}] \\right) \\qquad \\text{with} \\; n_{\\ell-1} \\ge n_\\ell\n",
    "$$\n",
    "with the matrix $\\hat{\\Sigma}_{n_\\ell,ij} [Y_{\\ell'}]$ defined $\\forall(i,j) \\in \\{1,\\dots,d\\}$ as :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\Sigma}_{n_\\ell,ij} [Y_{\\ell'}] & = \\frac{n_\\ell}{n_\\ell-1} \\sum_{k=1}^{n_\\ell}\\left[ \\left( Y_{\\ell',i}^{(k)} - \\hat{\\mathbb{E}}_{\\ell,i}[Y_{\\ell',i}^{(k)}] \\right) - \\left( Y_{\\ell',j}^{(k)} - \\hat{\\mathbb{E}}_{\\ell,j}[Y_{\\ell',j}^{(k)}] \\right) \\right] \\\\\n",
    "& = \\frac{n_\\ell}{n_\\ell-1} \\left[ \\sum_{k=1}^{n_\\ell} Y_{\\ell',i}^{(k)} Y_{\\ell',j}^{(k)} - \\left( \\sum_{k=1}^{n_\\ell} Y_{\\ell',i}^{(k)} \\right) \\left( \\sum_{k=1}^{n_\\ell} Y_{\\ell',j}^{(k)} \\right) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with $f_{\\ell'}(X^{(k)}) = \\left( Y_{\\ell',i}^{(k)}, Y_{\\ell',j}^{(k)} \\right)$ and $(k)$ the $k$-th element of our sample of size $n_\\ell$.\n",
    "\n",
    "In practice, we use the equation $(2)$ and if $Y$ is a centered vector, which means that $\\mathbb{E}_{\\ell,i}[Y^{(k)}] = 0$ for $ i \\in \\{1,\\dots,d\\}$, then the equation becomes : \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\Sigma}_{n_\\ell,ij} [Y_{\\ell'}] & = 1 \\cdot \\sum_{k=1}^{n_\\ell} Y_{\\ell',i}^{(k)} Y_{\\ell',j}^{(k)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with $\\frac{n_\\ell}{n_\\ell-1}$ becoming $\\frac{n_\\ell}{n_\\ell} = 1$\n",
    "\n",
    "Once we have those matricies, we can use the MLMC Euclidean Estimator $\\hat{\\Sigma}_{L}^{EMF}$ or use the MLMC Log-Euclidean Estimator $\\hat{\\Sigma}_{L}^{LEMF}$ (Eq. (7), [Maurais Aimee, Terrence Alsup, Benjamin Peherstorfer, and Youssef Marzouk](https://arxiv.org/pdf/2301.13749.pdf)) described in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MLMC Covariance Estimation in Log-Euclidean Geometry\n",
    "\n",
    "Based on the work of [Maurais Aimee, Terrence Alsup, Benjamin Peherstorfer, and Youssef Marzouk](https://arxiv.org/pdf/2301.13749.pdf) (Eq. (7)) we have the the MLMC Log-Euclidean Estimator $\\hat{\\Sigma}_{L}^{LEMF}$ :"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
