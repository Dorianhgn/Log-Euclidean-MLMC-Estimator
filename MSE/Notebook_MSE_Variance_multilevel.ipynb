{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "cp.fuse(np.float16)\n",
    "h = 1.\n",
    "\n",
    "# Get the current free memory on CUDA device\n",
    "free_memory = cp.cuda.Device().mem_info[0]\n",
    "\n",
    "def expected_var(v):\n",
    "    return cp.sum(cp.square(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v= cp.array([-0.00606533, -0.02837768, -0.20481078, -0.05524456,  0.00408442, -0.02378791, -0.11289296, -0.09047946, -0.0828985,   0.01015773]) # Real life\n",
    "v_0 = cp.array([-0.26251362, -0.22397083, -0.28459696, -0.14160629,  0.11507459,\n",
    "       -0.01314795,  0.00368215, -0.2233519 , -0.0494188 , -0.09833207])\n",
    "v_0 = cp.array([ 1.02602951,  1.59187996,  0.23337749, -0.62634188,  0.16052645,\n",
    "       -0.47135837,  0.21609987,  0.45909724,  0.70707697, -0.76436048])\n",
    "v_1 = cp.array([-0.09152057,  0.26501426, -0.26361748, -0.09584528,  0.07564258,\n",
    "       -0.28932995, -0.15172387, -0.17311338, -0.02250007, -0.02662765])\n",
    "v_2 = cp.array([ 0.02806491,  0.18164134, -0.02569311,  0.08406244,  0.09760685,\n",
    "       -0.2754576 , -0.18692242,  0.05429335, -0.05959692, -0.16104073])\n",
    "v_3 = cp.array([ 0.00719622, -0.01367537, -0.04378771,  0.15642576,  0.03295938,\n",
    "       -0.1364489 , -0.02709714, -0.16822205, -0.15617831, -0.05832736])\n",
    "\n",
    "v_array = [v_0, v_1, v_2, v_3]\n",
    "exp_var = cp.sum(cp.square(v_3))\n",
    "\n",
    "C_ell = cp.array([.25,0.5,0.75,1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Either the cell above or below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_1= cp.array([-0.00606533, -0.02837768, -0.20481078, -0.05524456,  0.00408442, -0.02378791, -0.11289296, -0.09047946, -0.0828985,   0.01015773])\n",
    "v_0 = cp.array([ 0.05289612, -0.03298811, -0.21346561, -0.03713753, -0.06898345,\n",
    "       -0.12286244, -0.07009093, -0.13369201, -0.19136073, -0.00077765])\n",
    "v_array = [v_0, v_1]\n",
    "exp_var = expected_var(v_1)\n",
    "print(\"diff =\",expected_var(v_1)-expected_var(v_0))\n",
    "\n",
    "C_ell = cp.array([0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_1 = cp.array([-0.04775578, -0.02158142, -0.22861181,  0.00999135 , 0.05261623, -0.03810132, -0.08892537, -0.09858141, -0.06851002,  0.09957945])\n",
    "v_2= cp.array([-0.00606533, -0.02837768, -0.20481078, -0.05524456,  0.00408442, -0.02378791, -0.11289296, -0.09047946, -0.0828985,   0.01015773]) #v_1 => high fidelity\n",
    "v_0= cp.array([-0.2506599 ,  0.05844203, -0.30284406,  0.08003633,  0.13573025, 0.04745837,  0.05874857,  0.07815323,  0.20496045,  0.16683998])\n",
    "\n",
    "v_array = [v_0, v_1, v_2]\n",
    "exp_var = expected_var(v_array[-1])\n",
    "C_ell = cp.array([.50,0.70,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating vectors to have a variance close to the real one v\n",
    "eps = 10e-2\n",
    "v_generated = v_1 + cp.random.normal(0,eps,len(v))\n",
    "print(\"|V - V_generated| =\", cp.linalg.norm(expected_var(v_generated) - expected_var(v)))\n",
    "print(\"exp_var v_generated =\" , expected_var(v_generated))\n",
    "print(\"diff =\" , expected_var(v_1)-expected_var(v_generated))\n",
    "v_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable & functions def: \n",
    "def f_vec(X,v):\n",
    "    return v.T@X\n",
    "\n",
    "def f_vec_squared(X,v):\n",
    "    return (v.T@X)**2\n",
    "\n",
    "d = 10 # lenght of random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the estimator of the variance in ML & LML :\n",
    "def estim_var_MLMC_tqdm(N, d, M, batch_size):\n",
    "    var_results = []\n",
    "    for i in tqdm(range(0, N, batch_size)):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        # Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        Var_MLMC_batch = cp.var(f_vec(X_M_ell,v_0), axis=1, ddof=1)\n",
    "        X_M_ell = None\n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            # Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            # Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            Var_MLMC_batch += cp.var(f_vec(X_M_ell,v_array[ell]), axis=1, ddof=1) - cp.var(f_vec(X_M_ell,v_array[ell-1]), axis=1, ddof=1)\n",
    "\n",
    "        var_results.extend(Var_MLMC_batch)\n",
    "\n",
    "        # Free memory\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results) # return var_results (len = N)\n",
    "\n",
    "def estim_var_MLMC_log_tqdm(N, d, M, batch_size):\n",
    "    var_results = []\n",
    "    for i in tqdm(range(0, N, batch_size)):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        # Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        log_Var_log_MLMC = cp.log(cp.var(f_vec(X_M_ell,v_0), axis=1, ddof=1))\n",
    "        X_M_ell = None\n",
    "    \n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            # Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            # Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            log_Var_log_MLMC += np.log(cp.var(f_vec(X_M_ell,v_array[ell]), axis=1, ddof=1)) - cp.log(cp.var(f_vec(X_M_ell,v_array[ell-1]), axis=1, ddof=1))\n",
    "\n",
    "        Var_MLMC_log_batch = cp.exp(log_Var_log_MLMC)\n",
    "    \n",
    "        var_results.extend(Var_MLMC_log_batch) \n",
    "\n",
    "        # Free memory if necessary\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results) # return var_results (len = N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the estimator of the variance in ML & LML :\n",
    "def estim_var_MLMC(N, d, M, batch_size):\n",
    "    var_results = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        # Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        Var_MLMC_batch = cp.var(f_vec(X_M_ell,v_0), axis=1, ddof=1)\n",
    "        X_M_ell = None\n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            # Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            # Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            Var_MLMC_batch += cp.var(f_vec(X_M_ell,v_array[ell]), axis=1, ddof=1) - cp.var(f_vec(X_M_ell,v_array[ell-1]), axis=1, ddof=1)\n",
    "\n",
    "        var_results.extend(Var_MLMC_batch)\n",
    "\n",
    "        # Free memory\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results) # return var_results (len = N)\n",
    "\n",
    "def estim_var_MLMC_log(N, d, M, batch_size):\n",
    "    var_results = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        # Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        log_Var_log_MLMC = cp.log(cp.var(f_vec(X_M_ell,v_0), axis=1, ddof=1))\n",
    "        X_M_ell = None\n",
    "    \n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            # Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            # Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            log_Var_log_MLMC += np.log(cp.var(f_vec(X_M_ell,v_array[ell]), axis=1, ddof=1)) - cp.log(cp.var(f_vec(X_M_ell,v_array[ell-1]), axis=1, ddof=1))\n",
    "\n",
    "        Var_MLMC_log_batch = cp.exp(log_Var_log_MLMC)\n",
    "    \n",
    "        var_results.extend(Var_MLMC_log_batch) \n",
    "\n",
    "        # Free memory if necessary\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results) # return var_results (len = N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we can't compute all at once due to M_0 and M_1 increasing, we need to separate the caluclations by computing batch_size first\n",
    "def get_max_batch_size(d, M, free_memory, memory_utilization=0.7):\n",
    "    \"\"\"\n",
    "    Calculate the maximum batch size directly based on the memory usage per data sample and the available memory.\n",
    "    \"\"\"\n",
    "    # Calculate the memory usage per sample\n",
    "    bytes_per_number = cp.dtype('float16').itemsize\n",
    "    omega = (d+1) * bytes_per_number * (np.sum(M))  # Total memory used per sample\n",
    "\n",
    "    # Calculate the maximum amount of memory available for batches\n",
    "    max_memory_per_batch = free_memory * memory_utilization\n",
    "\n",
    "    # Calculate the maximum batch size that can fit within the memory limit\n",
    "    batch_size = int(max_memory_per_batch / omega)  # Use int to ensure we get a whole number\n",
    "\n",
    "    return batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 1000000\n",
    "# M = [18]\n",
    "# X_M_ell = cp.random.standard_normal((N,d,M[0]))\n",
    "# Y_ell_M_ell_squared = f_vec_squared(X_M_ell,v)\n",
    "# Y_ell_M_ell = f_vec(X_M_ell,v)\n",
    "\n",
    "# print(cp.mean(Y_ell_M_ell))\n",
    "\n",
    "# print(\"Var en calculant avec cp.var (ddof = 1) =\", cp.var(Y_ell_M_ell, axis=1, ddof=1))\n",
    "# print(\"Var en calculant avec cp.var + correction =\", M[0]/(M[0]-1)*cp.var(Y_ell_M_ell, axis=1))\n",
    "# print(\"var en calculant à la main la somme =\", cp.sum(Y_ell_M_ell_squared, axis=1)/(M[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_ell_tab(eta, V_ell, C_ell,m=0):\n",
    "    \"\"\"\n",
    "    eta : budget\n",
    "    V_ell : tableau contenant [V_0,V_1,...,V_L]\n",
    "    C_ell : tableau contenant [C_0,C_1,...,C_L]\n",
    "\n",
    "    return : la formule M_ell au dessus qui nous donne le nombre d'élement optimal à simuler\n",
    "    \"\"\"\n",
    "    eta_tilde = eta # - m*cp.sum(...)\n",
    "    S_L = (cp.sqrt(cp.multiply(V_ell,C_ell))).sum()\n",
    "\n",
    "    return m + 1 + cp.floor(\n",
    "        (eta_tilde/S_L)*cp.sqrt(V_ell*1/C_ell)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombres de cas patho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cette cellule si t'as changé les C_ell\n",
    "N_pilote = 10000\n",
    "X = cp.random.standard_normal((d,N_pilote))\n",
    "V = [cp.var(f_vec_squared(X,v_0))]\n",
    "for i in range(1,len(v_array)):\n",
    "    V.extend([cp.var(f_vec_squared(X,v_array[i]) - f_vec_squared(X,v_array[i-1]))])\n",
    "\n",
    "V_ell = cp.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_eta_int = 1000\n",
    "N = 100000\n",
    "d = 10\n",
    "M = list(map(int,M_ell_tab(budget_eta_int,V_ell,C_ell)))\n",
    "# print(M)\n",
    "batch_size = get_max_batch_size(d, M, free_memory)\n",
    "print(batch_size)\n",
    "print(\"M_ell :\", M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab_var_MLMC = estim_var_MLMC_tqdm(N, d, M, batch_size)\n",
    "cp.mean(Tab_var_MLMC)\n",
    "#Tab_var_MLMC = np.array([x.item() for x in Tab_var_MLMC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab_var_log_MLMC = estim_var_MLMC_log_tqdm(N, d, M, batch_size)\n",
    "#Tab_var_log_MLMC = np.array([x.item() for x in Tab_var_log_MLMC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(cp.sum(Tab_var_MLMC_log < 0))\n",
    "# bias_MLMC_tab = []\n",
    "# tab_neg_val = []\n",
    "# bias_log_MLMC_tab = []\n",
    "# for _ in range(100):\n",
    "#     Tab_var_MLMC = estim_var_MLMC(N, d, M, batch_size)\n",
    "#     bias_MLMC_tab.append(cp.asnumpy(cp.abs(cp.mean(Tab_var_MLMC)-exp_var)))\n",
    "#     tab_neg_val.append(cp.asnumpy(cp.sum(Tab_var_MLMC < 0)))\n",
    "#     Tab_var_MLMC = None\n",
    "\n",
    "#     Tab_var_log_MLMC = estim_var_MLMC_log(N, d, M, batch_size)\n",
    "#     bias_log_MLMC_tab.append(cp.asnumpy(np.abs(np.mean(Tab_var_log_MLMC)-exp_var)))\n",
    "#     Tab_var_log_MLMC = None\n",
    "\n",
    "# print(f\"{np.mean(bias_MLMC_tab):.2e}\")\n",
    "# print(format(np.mean(bias_log_MLMC_tab), \".2e\"))\n",
    "# print(cp.mean(cp.array(tab_neg_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"$V^{ML}$ bias =\",f\"{cp.asnumpy(cp.abs(cp.mean(Tab_var_MLMC)-exp_var)):.2e}\")\n",
    "print(f\"{cp.mean(cp.asnumpy(cp.sum(Tab_var_MLMC < 0)))/N*100}\")\n",
    "print(\"$V^{ML}$ bias =\",f\"{cp.asnumpy(np.abs(np.mean(Tab_var_log_MLMC)-exp_var)):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f'save_temp_data_hist\\Tab_var_MLMC_eta_{budget_eta_int}', Tab_var_MLMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "\n",
    "# Histogramme et ligne de variance attendue\n",
    "ax1.hist(cp.asnumpy(Tab_var_MLMC), bins=30)\n",
    "ax1.set_xlim([-0.075, 0.25])\n",
    "ax1.set_ylim([0, 1330])\n",
    "ax1.axvline(x=cp.asnumpy(exp_var), color='r', linestyle='--', linewidth=1.25, label='Expected Variance')\n",
    "ax1.axvline(x=np.mean(cp.asnumpy(Tab_var_MLMC)), color='lime', linestyle='-', linewidth=1, label='Mean Value of $\\hat{V}^{ML}_L$')\n",
    "\n",
    "# Ajouter une zone hachurée\n",
    "ax1.axvspan(xmin=-0.075, xmax=0, ymin=0, ymax=1, facecolor='gray', alpha=0.2, hatch='//')\n",
    "\n",
    "ax2.hist(cp.asnumpy(Tab_var_log_MLMC), bins=30)\n",
    "ax2.set_xlim([-0.075, 0.25])\n",
    "ax2.set_ylim([0, 1330])\n",
    "ax2.axvline(x=cp.asnumpy(exp_var), color='r', linestyle='--', linewidth=1.25, label='Expected Variance')\n",
    "ax2.axvline(x=np.mean(cp.asnumpy(Tab_var_log_MLMC)), color='lime', linestyle='-', linewidth=1, label='Mean Value of $\\hat{V}^{LML}_L$')\n",
    "\n",
    "# Ajouter une zone hachurée\n",
    "ax2.axvspan(xmin=-0.075, xmax=0, ymin=0, ymax=1, facecolor='gray', alpha=0.2, hatch='//')\n",
    "\n",
    "\n",
    "# Ajout de légendes\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "# Ajout des x et y labels\n",
    "ax1.set_xlabel('$\\hat{V}^{ML}_L$')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.set_xlabel('$\\hat{V}^{LML}_L$')\n",
    "# ax2.set_ylabel('Count')\n",
    "\n",
    "# Ajout des titles\n",
    "ax1.set_title('Distribution of $\\hat{V}^{ML}_L$') \n",
    "ax2.set_title('Distribution of $\\hat{V}^{LML}_L$')\n",
    "\n",
    "fig.suptitle(r'Comparison of the distribution for estimates $\\hat{V}^{ML}_L$ and $\\hat{V}^{LML}_L$ for a low budget $B =$' + f'{budget_eta_int}')\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "\n",
    "# Histogramme et ligne de variance attendue\n",
    "ax1.hist(cp.asnumpy(Tab_var_MLMC), bins=30)\n",
    "ax1.set_xlim([-0.2, .325])\n",
    "ax1.set_ylim([0, 1920])\n",
    "ax1.axvline(x=cp.asnumpy(exp_var), color='r', linestyle='--', linewidth=1.25, label='Expected Variance')\n",
    "ax1.axvline(x=np.mean(cp.asnumpy(Tab_var_MLMC)), color='lime', linestyle='-', linewidth=1, label='Mean Value of $\\hat{V}^{ML}_L$')\n",
    "\n",
    "# Ajouter une zone hachurée\n",
    "ax1.axvspan(xmin=-0.2, xmax=0, ymin=0, ymax=1, facecolor='gray', alpha=0.2, hatch='//')\n",
    "\n",
    "ax2.hist(cp.asnumpy(Tab_var_log_MLMC), bins=30)\n",
    "ax2.set_xlim([-0.2, .325])\n",
    "ax2.set_ylim([0, 1920])\n",
    "ax2.axvline(x=cp.asnumpy(exp_var), color='r', linestyle='--', linewidth=1.25, label='Expected Variance')\n",
    "ax2.axvline(x=np.mean(cp.asnumpy(Tab_var_log_MLMC)), color='lime', linestyle='-', linewidth=1, label='Mean Value of $\\hat{V}^{LML}_L$')\n",
    "\n",
    "# Ajouter une zone hachurée\n",
    "ax2.axvspan(xmin=-0.2, xmax=0, ymin=0, ymax=1, facecolor='gray', alpha=0.2, hatch='//')\n",
    "\n",
    "\n",
    "# Ajout de légendes\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "# Ajout des x et y labels\n",
    "ax1.set_xlabel('$\\hat{V}^{ML}_L$')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.set_xlabel('$\\hat{V}^{LML}_L$')\n",
    "# ax2.set_ylabel('Count')\n",
    "\n",
    "# Ajout des titles\n",
    "ax1.set_title('Distribution of $\\hat{V}^{ML}_L$') \n",
    "ax2.set_title('Distribution of $\\hat{V}^{LML}_L$')\n",
    "\n",
    "fig.suptitle(r'Comparison of the distribution for estimates $\\hat{V}^{ML}_L$ and $\\hat{V}^{LML}_L$ for a low budget $B =$' + f'{budget_eta_int}')\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab_var_MLMC_log = estim_var_MLMC_log(N, d, M, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab_var_MLMC_log = [x.item() for x in Tab_var_MLMC_log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Tab_var_MLMC_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE en fonction de n\n",
    "budget fixé à $10^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_eta_int = 1e3\n",
    "samples_N = list(map(int,np.logspace(2,5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_tab = []\n",
    "var_MLMC_tab = []\n",
    "bias_log_tab = []\n",
    "var_MLMC_log_tab = []\n",
    "\n",
    "\n",
    "for N in tqdm(samples_N):\n",
    "    # Pour chaque budget eta, calcul de nos M_0 et M_1\n",
    "    M = list(map(int,M_ell_tab(budget_eta_int,V_ell,C_ell)))\n",
    "\n",
    "    # Compute the batch size\n",
    "    batch_size = get_max_batch_size(d, M, free_memory)\n",
    "    if batch_size == 0:\n",
    "        break  # Exit if batch size is too small\n",
    "    \n",
    "    # Calcul de nos tableaux de nos estimateurs de taille N\n",
    "    Tab_var_MLMC = estim_var_MLMC(N, d, M, batch_size)\n",
    "    var_MLMC_tab.append(cp.var(Tab_var_MLMC))\n",
    "    bias_tab.append(cp.square(cp.mean(Tab_var_MLMC) - expected_var(v)))\n",
    "\n",
    "    Tab_var_MLMC = None\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    Tab_var_MLMC_log = estim_var_MLMC_log(N, d, M, batch_size)\n",
    "    var_MLMC_log_tab.append(cp.var(Tab_var_MLMC_log))\n",
    "    bias_log_tab.append(cp.square(cp.mean(Tab_var_MLMC_log) - expected_var(v)))\n",
    "\n",
    "    Tab_var_MLMC_log = None\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "var_MLMC_tab = [x.item() for x in var_MLMC_tab]\n",
    "bias_tab = [x.item() for x in bias_tab]\n",
    "\n",
    "var_MLMC_log_tab = [x.item() for x in var_MLMC_log_tab]\n",
    "bias_log_tab = [x.item() for x in bias_log_tab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log-log\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot des données\n",
    "plt.loglog(samples_N, bias_tab, marker='o', label='MSE MLMC')\n",
    "#plt.loglog(samples_N, var_MLMC_tab, marker='s', label='Var MLMC')\n",
    "plt.loglog(samples_N, bias_log_tab, marker='^', label='MSE Log-MLMC', c='g')\n",
    "#plt.loglog(samples_N, var_MLMC_log_tab, marker='x', label='Var Log-MLMC')\n",
    "\n",
    "\n",
    "# Ajouter titre et légendes\n",
    "plt.title(f'MSE et variance en fonction des N pour budget={budget_eta_int}')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('MSE/Variance')\n",
    "plt.legend()\n",
    "\n",
    "# Afficher le plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teta_hat = cp.random.exponential(30,10)\n",
    "cp.mean(cp.log(teta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.log(cp.mean(teta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.random.exponential(2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation du Bootstrap dans le calcul à chaque niveau\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbb V = \\log \\left( x \\right)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bootstrap_var(Y, num_samples):\n",
    "    \"\"\"\n",
    "    This function computes the variance of Y using the bootstrap method.\n",
    "    It creates num_samples of variances by taking randomly M_ell (= len(Y)) values in Y, and then returning its variance.\n",
    "    Then it return the expectation of this variances\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: numpy array, values that we want the variance\n",
    "    - num_samples: int, the number of bootstrap samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    - int, the expectation of bootstrap variances \n",
    "    \"\"\"\n",
    "    N, M_ell = cp.shape(Y)\n",
    "    bootstrap_variances = cp.empty((N,num_samples))\n",
    "    for i in range(num_samples):\n",
    "        sample_indices = np.random.randint(0, M_ell, M_ell)\n",
    "        bootstrap_Y = Y[:, sample_indices]\n",
    "        bootstrap_variances[:,i] = cp.var(bootstrap_Y, axis=1, ddof=1) # corrected empirical variance\n",
    "\n",
    "    return cp.mean(bootstrap_variances, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_bootstrap_var(Y, num_samples):\n",
    "    \"\"\"\n",
    "    This function computes the variance of Y using the bootstrap method, vectorized to avoid explicit loops.\n",
    "    It creates num_samples of variances by taking randomly M_ell (= len(Y)) values in Y, and then returns its variance.\n",
    "    Then it returns the expectation of these variances.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: numpy array, values for which we want the variance\n",
    "    - num_samples: int, the number of bootstrap samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array, the expectation of bootstrap variances for each feature\n",
    "    \"\"\"\n",
    "    N, M_ell = Y.shape\n",
    "    # Generate random indices for all bootstrap samples at once\n",
    "    sample_indices = np.random.randint(0, M_ell, (M_ell, num_samples))\n",
    "    # Index Y to create the bootstrap samples: (N, M_ell, num_samples)\n",
    "    bootstrap_Y = Y[:, sample_indices]\n",
    "    # Compute variances across the middle dimension, for each sample\n",
    "    bootstrap_variances = np.var(bootstrap_Y, axis=1, ddof=1)\n",
    "    # Compute the mean of the variances across samples\n",
    "    mean_variances = np.mean(bootstrap_variances, axis=1)\n",
    "    return mean_variances\n",
    "\n",
    "# Uncomment the following line to test the function with example data\n",
    "# vectorized_bootstrap_var(np.random.rand(10, 100), 1000)  # Example call with random data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the estimator of the variance in ML & LML :\n",
    "def Estim_var_MLMC_bootstrap(N, d, M, batch_size, bootstrap_sample_size=1000):\n",
    "    var_results = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        Var_MLMC_batch = Bootstrap_var(Y_ell_M_ell,bootstrap_sample_size)\n",
    "    \n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            V_ell_M_ell_bootstrap = Bootstrap_var(Y_ell_M_ell,bootstrap_sample_size)\n",
    "\n",
    "            Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            V_ell_minus_1_M_ell_bootstrap = Bootstrap_var(Y_ell_minus_1_M_ell,bootstrap_sample_size)\n",
    "\n",
    "            Var_MLMC_batch += V_ell_M_ell_bootstrap - V_ell_minus_1_M_ell_bootstrap\n",
    "\n",
    "        var_results.extend(Var_MLMC_batch)\n",
    "\n",
    "        # Free memory\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results)\n",
    "\n",
    "def Estim_var_MLMC_log(N, d, M, batch_size, bootstrap_sample_size):\n",
    "    var_results = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        actual_batch_size = min(batch_size, N - i)\n",
    "        # Variance at level 0\n",
    "        X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[0]))\n",
    "        Y_ell_M_ell = f_vec(X_M_ell,v_0)\n",
    "        log_Var_log_MLMC = cp.log(cp.var(Y_ell_M_ell, axis=1, ddof=1))\n",
    "    \n",
    "        # Loop on M to have correction V^(ell)_{M_ell}(Y_ell) - V^(ell)_{M_ell}(Y_{ell-1})\n",
    "        for ell in range(1,len(M)):\n",
    "            X_M_ell = cp.random.standard_normal((actual_batch_size,d,M[ell]))\n",
    "            Y_ell_M_ell = f_vec(X_M_ell,v_array[ell])\n",
    "            Y_ell_minus_1_M_ell = f_vec(X_M_ell,v_array[ell-1])\n",
    "            log_Var_log_MLMC += np.log(cp.var(Y_ell_M_ell, axis=1, ddof=1)) - cp.log(cp.var(Y_ell_minus_1_M_ell, axis=1, ddof=1))\n",
    "\n",
    "        Var_MLMC_log_batch = cp.exp(log_Var_log_MLMC)\n",
    "    \n",
    "        var_results.extend(Var_MLMC_log_batch)\n",
    "\n",
    "        # Free memory if necessary\n",
    "        X_M_ell = None\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    return cp.array(var_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "budget_eta_int_bootstrap = 1e3\n",
    "M = list(map(int,M_ell_tab(budget_eta_int_bootstrap,V_ell,C_ell)))\n",
    "\n",
    "batch_size = get_max_batch_size(d, M, free_memory)\n",
    "if batch_size == 0:\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "Tab_var_MLMC = estim_var_MLMC(N, d, M, batch_size)\n",
    "\n",
    "Tab_var_MLMC_bootstrap = Estim_var_MLMC_bootstrap(N, d, M, batch_size, bootstrap_sample_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab_var_MLMC.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tab_var_MLMC_bootstrap.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_var(v_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet4A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
